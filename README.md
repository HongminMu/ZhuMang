# **Wearable Visuomotor Navigation and Mobility Assistance Device** 👓  
### **For the Partially Sighted and Visually Impaired in Unfamiliar Indoor Scenes**  

![Device Image](#)  <!-- Insert a relevant image of your wearable assistive device here -->

## **🏆 Awards & Achievements**  
This project was highly recognized in two prestigious robotics competitions:  

- 🥇 **First Prize** - **World Robot Contest - Tri-Co Robots Challenge** (August 2024)  
  - **Ranked Top 3 out of 19 teams**  
  - **Competition Project Name**: *OpenAEye: an Assistive Glasses for the Visually Impaired*  
- 🥈 **National Runner-up** - **China Graduate Robotics Innovation Design Contest** (September 2023)  
  - **Ranked 2nd out of 1,177 teams**  
  - **Competition Project Name**: *Wearable Assistive Device for the Visually Impaired to Navigate in Unfamiliar Indoor Environments*  

## **🌟 Project Overview**  
This project, formally titled **"Wearable Visuomotor Navigation and Mobility Assistance Device for the Partially Sighted and Visually Impaired in Unfamiliar Indoor Scenes"**, enhances **mobility and independence** for visually impaired individuals by enabling **navigation in unfamiliar indoor environments without relying on dense pre-built maps**.  

🚀 **Key Features:**  
✅ **SLAM-free navigation** – Uses **Topological Semantic Mapping (TSM)** instead of conventional dense mapping.  
✅ **Text-based Localization** – Matches detected text from environmental signage with TSM nodes for precise positioning.  
✅ **Binocular Stereo Vision** – Enhances depth estimation to assist in navigation toward key landmarks.  
✅ **AI-Driven Obstacle Detection** – Uses deep learning for real-time scene awareness.  
✅ **Multi-Device Wearable Interaction** – Smart glasses process visual data, while wristbands provide haptic feedback.  

---

## **🏆 1️⃣ World Robot Contest - Tri-Co Robots Challenge (First Prize 🥇, Top 3/19)**  
At **World Robot Contest**, our project was presented under the name **"OpenAEye: an Assistive Glasses for the Visually Impaired"**, focusing on wearable **real-time visuomotor perception and semantic scene understanding**.  

📌 **Key Innovations:**  
- **TSM-based navigation**, reducing reliance on pre-built dense maps.  
- **Signage-based localization**, enabling PSVI users to position themselves accurately.  
- **Stereo vision depth sensing**, improving obstacle detection and environmental awareness.  

📸 **Competition Showcase:**  
- **Volunteer Testing**  
  ![image](https://github.com/HongminMu/ZhuMang/assets/57067148/7820972f-91ab-4a45-aa9f-684060dc663b)  
- **Device Prototype**  
  <img width="1278" alt="111" src="https://github.com/user-attachments/assets/e6408f10-d5b1-4d98-bc37-b9c6c714f3fc">  
- **Live Demonstration**  
  *(Video coming soon! 🎥)*  

---

## **🏆 2️⃣ China Graduate Robotics Innovation Design Contest (National Runner-up 🥈, 2nd/1,177)**  
In this competition, our project was presented as **"Wearable Assistive Device for the Visually Impaired to Navigate in Unfamiliar Indoor Environments"**, emphasizing **real-time multi-modal environmental perception and wearable human-computer interaction**.  

📌 **Key Achievements:**  
- **Hybrid Sensor Fusion** – Integrates RGB, IMU, and stereo vision for robust scene understanding.  
- **Dynamic Obstacle Avoidance** – Uses real-time segmentation to ensure safe mobility.  
- **User-Centric Wearable Design** – Compact, lightweight, and intuitive for PSVI users.  

📸 **Competition Showcase:**  
- **Indoor Navigation Demo** *(Insert image here)*  
- **System Components & Architecture** *(Insert image here)*  
- **User Interaction Test** *(Insert image here)*  

---

## **🎥 Video Demonstration**  
You can download the demo videos here! *(within 25MB each)*.  

## **🚀 Code Release**  
Our code will be made publicly available soon! Stay tuned for updates. 🌍💡  

---

## **📚 Related Work**  
This work builds upon our previous research on mobility assistance systems for visually impaired users. Specifically, we extend our prior work on **dynamic obstacle avoidance** using **instance segmentation** to improve the **navigation experience in indoor environments**.  

You can find our previous work referenced below:  

> **Mu, Hongmin and others**, *Dynamic Obstacle Avoidance System Based on Rapid Instance Segmentation Network*, **IEEE Trans. on Intelligent Transportation Systems**, **2024**, **Vol. 25**, **No. 5**, **Pages 4578-4592**, [DOI: 10.1109/TITS.2023.3323210](https://doi.org/10.1109/TITS.2023.3323210).  

```bibtex
@ARTICLE{Mu2024,
  author={Mu, Hongmin and others},
  journal={IEEE Trans. on Intelligent Transportation Systems}, 
  title={Dynamic Obstacle Avoidance System Based on Rapid Instance Segmentation Network}, 
  year={2024},
  volume={25},
  number={5},
  pages={4578-4592},
  keywords={Feature extraction;Task analysis;Collision avoidance;Real-time systems;Distance measurement;Cameras;Semantics;Obstacle avoidance;instance segmentation;mobility assistance;indoor navigation},
  doi={10.1109/TITS.2023.3323210}
}

# **Wearable Visuomotor Navigation and Mobility Assistance Device for the Partially Sighted and Visually Impaired in Unfamiliar Indoor Scenes** ðŸ‘“ 

## **ðŸŒŸ Project Overview**  
This project, formally titled **"Wearable Visuomotor Navigation and Mobility Assistance Device for the Partially Sighted and Visually Impaired in Unfamiliar Indoor Scenes"**, enhances **mobility and independence** for visually impaired individuals by enabling **navigation in unfamiliar indoor environments without relying on dense pre-built maps**.  

Overview of our device and navigation system :
![12334](https://github.com/user-attachments/assets/a1a99db3-299d-4ad8-83e5-a10197fc02fd)

ðŸš€ **Key Features:**  
âœ… **SLAM-free navigation** â€“ Uses **Topological Semantic Mapping (TSM)** instead of conventional dense mapping.  
âœ… **Text-based Localization** â€“ Matches detected text from environmental signage with TSM nodes for precise positioning.  
âœ… **Binocular Stereo Vision** â€“ Enhances depth estimation to assist in navigation toward key landmarks.  
âœ… **AI-Driven Obstacle Detection** â€“ Uses deep learning for real-time scene awareness.  
âœ… **Multi-Device Wearable Interaction** â€“ Smart glasses process visual data, while wristbands provide haptic feedback.  

---

## **ðŸ† Awards & Achievements**  
This project was highly recognized in two prestigious robotics competitions:

- ðŸ¥‡ **First Prize** - **World Robot Contest - Tri-Co Robots Challenge** (August 2024)  
  - **Ranked Top 3 out of 19 teams**  
  - **Project Name**: *OpenAEye: an Assistive Glasses for the Visually Impaired*  

![image](https://github.com/user-attachments/assets/05ec16c9-2c00-4bf0-80ff-181da953cfa5)

- ðŸ¥ˆ **National Runner-up** - **China Graduate Robotics Innovation Design Contest** (September 2023)  
  - **Ranked 2nd out of 1,177 teams**  
  - **Project Name**: *Wearable Assistive Device for the Visually Impaired to Navigate in Unfamiliar Indoor Environments*
 
![333](https://github.com/user-attachments/assets/1ca68f01-71f6-4268-87a5-5b560f90ba76)

## **ðŸŽ¥ Video Demonstration**  
You can download the demo videos *(within 25MB each)* here!

[Volunteer 1](https://github.com/HongminMu/ZhuMang/blob/main/volunteer1.mp4)  
[Volunteer 2](https://github.com/HongminMu/ZhuMang/blob/main/volunteer2.mp4)

Volunteer 1
![image](https://github.com/HongminMu/ZhuMang/assets/57067148/7820972f-91ab-4a45-aa9f-684060dc663b)

Volunteer 2
![image](https://github.com/user-attachments/assets/9a6beec2-f583-49e0-ba18-19e6ffacf9b7)

## **ðŸš€ Code Release**  
Our code will be made publicly available soon! Stay tuned for updates. ðŸŒðŸ’¡  

## **ðŸ“© Contact**  
For inquiries, feel free to reach out via email at **hongmin_@163.com**, specifying the purpose of your request.  
We can provide hardware specifications and software code. Contributions, discussions, and collaborations are highly welcome!  

## **ðŸ“š Related Work**  
This work builds upon our previous research on obstacle avoidance systems for visually impaired users. Specifically, we extend our prior work on **dynamic obstacle avoidance** using **instance segmentation** to improve the **navigation experience in indoor environments**.  

You can find our previous work referenced below:  

> **Mu, Hongmin and others**, *Dynamic Obstacle Avoidance System Based on Rapid Instance Segmentation Network*, **IEEE Trans. on Intelligent Transportation Systems**, **2024**, **Vol. 25**, **No. 5**, **Pages 4578-4592**, [DOI: 10.1109/TITS.2023.3323210](https://doi.org/10.1109/TITS.2023.3323210).  

```bibtex
@ARTICLE{Mu2024Dynamic,
  author={Mu, Hongmin and Zhang, Gang and Ma, Zhe and Zhou, Mengchu and Cao, Zhengcai},
  journal={IEEE Trans. on Intelligent Transportation Systems}, 
  title={Dynamic Obstacle Avoidance System Based on Rapid Instance Segmentation Network}, 
  year={2024},
  volume={25},
  number={5},
  pages={4578-4592},
  keywords={Feature extraction;Task analysis;Collision avoidance;Real-time systems;Distance measurement;Cameras;Semantics;Obstacle avoidance;instance segmentation;mobility assistance;indoor navigation},
  doi={10.1109/TITS.2023.3323210}
}

